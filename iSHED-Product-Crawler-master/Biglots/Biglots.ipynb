{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "# product_details = {}\n",
    "def Scrape(url,Category,sub_name):\n",
    "    time.sleep(2)\n",
    "    url = url\n",
    "    Category=Category\n",
    "    print(Category)\n",
    "    ua = UserAgent()\n",
    "    header  = {'User-Agent':str(ua)}\n",
    "#     result = requests.get(url)\n",
    "    result=requests.get(url,headers = header, timeout = 20)\n",
    "    soap = BeautifulSoup(result.content , 'lxml')\n",
    "    product_detals = {}\n",
    "    for i in soap.find_all('div',{'class':'product-detail-wrapper'}):\n",
    "        try:\n",
    "            product_detals['Name'] = i.find('h1').text\n",
    "        except:\n",
    "            product_detals['Name']='Nill'\n",
    "    try:        \n",
    "        product_detals['Price'] = i.find(class_ = 'regular-price').text.strip()\n",
    "    except:\n",
    "        product_detals['Price'] ='Nill'\n",
    "#     Sku = soap.find('div',{'class':'product-sku'}).text.strip()\n",
    "    \n",
    "    try:\n",
    "        model = soap.find_all('div',{'class':'accordion-body-content'})\n",
    "        product_detals['Model_number']= model[-3].find('li').text.strip()\n",
    "    except :\n",
    "        product_detals['Model_number'] ='Nill'\n",
    "    try:\n",
    "        \n",
    "        product_detals['Description']=soap.find('p',{'class':'product-short-description'}).text.strip()\n",
    "    except :\n",
    "        product_detals['Description'] = 'Nill'\n",
    "    try:\n",
    "        product_detals['Category']=Category\n",
    "    except:\n",
    "        product_detals['Category'] ='Nill'\n",
    "    try:\n",
    "        product_detals['Image']=soap.find('img',{'class':'viewer-main-image'}).get('srcset').strip('//')\n",
    "    except:\n",
    "        product_detals['Image'] ='Nill'\n",
    "    print(product_detals)\n",
    "    product_csv_creator_file(product_detals, sub_name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import os.path\n",
    "import io\n",
    "import datetime\n",
    "today_date = datetime.date.today()\n",
    "\n",
    "\n",
    "def product_csv_creator_file(data, category_name):\n",
    "\n",
    "    if not os.path.exists('product_csv/'+str(today_date)+'/'):\n",
    "        os.makedirs('product_csv/'+str(today_date)+'/')\n",
    "    filename = 'product_csv/'+str(today_date)+'/'+category_name+'.csv'\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    with io.open(filename, 'a', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Category','Sub-category' ,'price','Image','Model_number','Description']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        writer.writerows([{'Category':data['Category'],'Sub-category':data['Name'] ,'price':data['Price'],\n",
    "                           'Image':data['Image'],'Model_number':data['Model_number'],\n",
    "                           'Description':data['Description']}])\n",
    "                           \n",
    "        print(\"Writing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Search_url(Category,name):\n",
    "    sub_name = name\n",
    "    Category = Category\n",
    "    print(sub_name)\n",
    "    url ='https://www.biglots.com/search/?Ntt='+ str(sub_name)\n",
    "    base_url = 'https://www.biglots.com'\n",
    "    print(url)\n",
    "    page_response = requests.get(url,timeout=10)\n",
    "    bs = BeautifulSoup(page_response.content, 'lxml')    \n",
    "    links=[]\n",
    "    for link in bs.find_all('a',{'class':'product-link'}):\n",
    "#         print(link.get('href'))\n",
    "        links.append(link.get('href'))\n",
    "    for link in links:\n",
    "        complete = base_url+link\n",
    "        print(complete)\n",
    "        Scrape(complete,Category,sub_name)\n",
    "        time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
